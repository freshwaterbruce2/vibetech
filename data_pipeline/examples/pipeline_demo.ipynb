{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing Pipeline Demo\n",
    "\n",
    "This notebook demonstrates the complete data processing pipeline with real examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Import pipeline modules\n",
    "from core.pipeline import DataPipeline\n",
    "from core.config import PipelineConfig, TransformConfig, ValidationConfig\n",
    "from monitoring.visualizer import QualityVisualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "n_samples = 1000\n",
    "sample_data = pd.DataFrame({\n",
    "    'customer_id': range(1, n_samples + 1),\n",
    "    'age': np.random.randint(18, 80, n_samples),\n",
    "    'income': np.random.lognormal(10.5, 0.5, n_samples),\n",
    "    'purchase_amount': np.random.lognormal(4, 1.5, n_samples),\n",
    "    'category': np.random.choice(['Electronics', 'Clothing', 'Food', 'Books', 'Sports'], n_samples),\n",
    "    'rating': np.random.uniform(1, 5, n_samples),\n",
    "    'signup_date': pd.date_range('2023-01-01', periods=n_samples, freq='H'),\n",
    "    'city': np.random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'], n_samples),\n",
    "    'has_premium': np.random.choice([0, 1], n_samples, p=[0.7, 0.3])\n",
    "})\n",
    "\n",
    "# Add some missing values\n",
    "missing_mask = np.random.random(n_samples) < 0.05\n",
    "sample_data.loc[missing_mask, 'rating'] = np.nan\n",
    "\n",
    "# Add some duplicates\n",
    "duplicates = sample_data.sample(20)\n",
    "sample_data = pd.concat([sample_data, duplicates], ignore_index=True)\n",
    "\n",
    "# Save to CSV\n",
    "sample_data.to_csv('sample_data.csv', index=False)\n",
    "print(f\"Created sample dataset with {len(sample_data)} rows and {len(sample_data.columns)} columns\")\n",
    "sample_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline configuration\n",
    "config = PipelineConfig(\n",
    "    name=\"Customer Analytics Pipeline\",\n",
    "    source_type=\"csv\",\n",
    "    source_path=\"sample_data.csv\",\n",
    "    output_path=\"processed_data.csv\",\n",
    "    enable_monitoring=True,\n",
    "    enable_profiling=True,\n",
    "    chunk_size=500\n",
    ")\n",
    "\n",
    "# Transformation configuration\n",
    "transform_config = TransformConfig(\n",
    "    handle_missing=\"median\",\n",
    "    remove_outliers=True,\n",
    "    scale_numeric=True,\n",
    "    encode_categorical=True\n",
    ")\n",
    "\n",
    "# Validation configuration\n",
    "validation_config = ValidationConfig(\n",
    "    check_schema=True,\n",
    "    check_quality=True,\n",
    "    quality_checks=[\n",
    "        \"missing_values\",\n",
    "        \"duplicates\",\n",
    "        \"outliers\",\n",
    "        \"data_types\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Pipeline configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Basic Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and run pipeline\n",
    "pipeline = DataPipeline(config)\n",
    "\n",
    "# Execute pipeline\n",
    "result = pipeline.execute(\n",
    "    transform_config=transform_config,\n",
    "    validation_config=validation_config\n",
    ")\n",
    "\n",
    "# Display execution summary\n",
    "if result['success']:\n",
    "    print(\"[SUCCESS] Pipeline executed successfully\")\n",
    "    print(f\"\\nProcessing Summary:\")\n",
    "    print(f\"- Original shape: {result['original_shape']}\")\n",
    "    print(f\"- Final shape: {result['final_shape']}\")\n",
    "    print(f\"- Rows processed: {result['rows_processed']}\")\n",
    "    print(f\"- Execution time: {result['execution_time']:.2f} seconds\")\n",
    "    \n",
    "    if 'validation_report' in result:\n",
    "        print(f\"\\nValidation Status: {'PASSED' if result['validation_report']['valid'] else 'FAILED'}\")\n",
    "else:\n",
    "    print(\"[ERROR] Pipeline failed:\")\n",
    "    for error in result.get('errors', []):\n",
    "        print(f\"  - {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Validation Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "processed_df = pd.read_csv('processed_data.csv')\n",
    "\n",
    "# Visualize validation results\n",
    "if result.get('validation_report'):\n",
    "    visualizer = QualityVisualizer()\n",
    "    visualizer.plot_validation_results(result['validation_report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.feature_engineer import FeatureEngineer\n",
    "\n",
    "# Create feature engineer\n",
    "engineer = FeatureEngineer(transform_config)\n",
    "\n",
    "# Define custom feature specifications\n",
    "feature_specs = [\n",
    "    {\n",
    "        'type': 'interaction',\n",
    "        'column1': 'age',\n",
    "        'column2': 'income',\n",
    "        'operation': 'multiply'\n",
    "    },\n",
    "    {\n",
    "        'type': 'transform',\n",
    "        'column': 'purchase_amount',\n",
    "        'transform': 'log'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Apply feature engineering\n",
    "df_with_features = engineer.engineer(sample_data.copy(), feature_specs)\n",
    "\n",
    "print(f\"Original features: {len(sample_data.columns)}\")\n",
    "print(f\"After engineering: {len(df_with_features.columns)}\")\n",
    "print(f\"\\nNew features created:\")\n",
    "new_features = set(df_with_features.columns) - set(sample_data.columns)\n",
    "for feat in list(new_features)[:10]:\n",
    "    print(f\"  - {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display performance metrics\n",
    "if result.get('monitoring_metrics'):\n",
    "    metrics = result['monitoring_metrics']\n",
    "    \n",
    "    print(\"Pipeline Performance Metrics:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Stage performance\n",
    "    if 'stages' in metrics:\n",
    "        print(\"\\nStage Durations:\")\n",
    "        for stage, stats in metrics['stages'].items():\n",
    "            print(f\"  {stage}: {stats.get('duration', 0):.2f}s\")\n",
    "    \n",
    "    # Resource usage\n",
    "    if 'summary' in metrics:\n",
    "        summary = metrics['summary']\n",
    "        print(f\"\\nResource Usage:\")\n",
    "        print(f\"  Peak Memory: {summary.get('peak_memory_mb', 0):.1f} MB\")\n",
    "        print(f\"  Avg CPU: {summary.get('average_cpu_percent', 0):.1f}%\")\n",
    "    \n",
    "    # Issues\n",
    "    print(f\"\\nIssues Encountered:\")\n",
    "    print(f\"  Errors: {len(metrics.get('errors', []))}\")\n",
    "    print(f\"  Warnings: {len(metrics.get('warnings', []))}\")\n",
    "    \n",
    "    # Visualize pipeline execution\n",
    "    visualizer = QualityVisualizer()\n",
    "    visualizer.plot_pipeline_summary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Profiling and Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile the processed data\n",
    "visualizer = QualityVisualizer()\n",
    "\n",
    "# Create data profiling visualization\n",
    "print(\"Data Profiling:\")\n",
    "visualizer.plot_data_profiling(processed_df, max_cols=12)\n",
    "\n",
    "# Correlation matrix for numeric features\n",
    "print(\"\\nCorrelation Analysis:\")\n",
    "visualizer.plot_correlation_matrix(processed_df, max_features=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Pipeline with Custom Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom transformation function\n",
    "def custom_age_group(df):\n",
    "    \"\"\"Create age group categories.\"\"\"\n",
    "    bins = [0, 25, 35, 50, 65, 100]\n",
    "    labels = ['Gen Z', 'Millennial', 'Gen X', 'Boomer', 'Senior']\n",
    "    df['age_group'] = pd.cut(df['age'], bins=bins, labels=labels)\n",
    "    return df\n",
    "\n",
    "def custom_revenue_segment(df):\n",
    "    \"\"\"Segment customers by revenue.\"\"\"\n",
    "    df['revenue_segment'] = pd.qcut(df['purchase_amount'], \n",
    "                                    q=[0, 0.25, 0.75, 1],\n",
    "                                    labels=['Low', 'Medium', 'High'])\n",
    "    return df\n",
    "\n",
    "# Configure pipeline with custom transformations\n",
    "advanced_config = PipelineConfig(\n",
    "    name=\"Advanced Customer Pipeline\",\n",
    "    source_type=\"csv\",\n",
    "    source_path=\"sample_data.csv\",\n",
    "    output_path=\"advanced_processed.csv\",\n",
    "    custom_transforms=[custom_age_group, custom_revenue_segment],\n",
    "    enable_monitoring=True\n",
    ")\n",
    "\n",
    "# Run advanced pipeline\n",
    "advanced_pipeline = DataPipeline(advanced_config)\n",
    "advanced_result = advanced_pipeline.execute(\n",
    "    transform_config=transform_config,\n",
    "    validation_config=validation_config\n",
    ")\n",
    "\n",
    "# Check new features\n",
    "advanced_df = pd.read_csv('advanced_processed.csv')\n",
    "print(\"Custom features added:\")\n",
    "print(advanced_df[['age', 'age_group', 'purchase_amount', 'revenue_segment']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Batch Processing Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple sample files\n",
    "for i in range(3):\n",
    "    batch_data = sample_data.sample(n=300)\n",
    "    batch_data.to_csv(f'batch_{i}.csv', index=False)\n",
    "\n",
    "# Process multiple files\n",
    "from core.pipeline import BatchProcessor\n",
    "\n",
    "batch_processor = BatchProcessor(config)\n",
    "batch_files = ['batch_0.csv', 'batch_1.csv', 'batch_2.csv']\n",
    "\n",
    "batch_results = []\n",
    "for file in batch_files:\n",
    "    config.source_path = file\n",
    "    config.output_path = f'processed_{file}'\n",
    "    \n",
    "    pipeline = DataPipeline(config)\n",
    "    result = pipeline.execute(\n",
    "        transform_config=transform_config,\n",
    "        validation_config=validation_config\n",
    "    )\n",
    "    batch_results.append(result)\n",
    "    print(f\"Processed {file}: {result['success']}\")\n",
    "\n",
    "# Aggregate results\n",
    "total_rows = sum(r['rows_processed'] for r in batch_results)\n",
    "avg_time = np.mean([r['execution_time'] for r in batch_results])\n",
    "print(f\"\\nBatch Processing Summary:\")\n",
    "print(f\"  Total rows processed: {total_rows}\")\n",
    "print(f\"  Average processing time: {avg_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save monitoring report\n",
    "if result.get('monitoring_metrics'):\n",
    "    from monitoring.monitor import PipelineMonitor\n",
    "    import json\n",
    "    \n",
    "    # Save metrics to JSON\n",
    "    with open('pipeline_metrics.json', 'w') as f:\n",
    "        json.dump(result['monitoring_metrics'], f, indent=2, default=str)\n",
    "    \n",
    "    print(\"Metrics saved to pipeline_metrics.json\")\n",
    "\n",
    "# Generate performance report\n",
    "if 'pipeline_monitor' in dir():\n",
    "    report = pipeline_monitor.generate_performance_report()\n",
    "    with open('performance_report.txt', 'w') as f:\n",
    "        f.write(report)\n",
    "    print(\"Performance report saved to performance_report.txt\")\n",
    "\n",
    "# Clean up temporary files\n",
    "import os\n",
    "for file in ['sample_data.csv', 'processed_data.csv', 'advanced_processed.csv'] + batch_files:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "print(\"\\nCleanup completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}