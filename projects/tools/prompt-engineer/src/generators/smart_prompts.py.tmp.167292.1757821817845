#!/usr/bin/env python3
"""
Enhanced Smart Prompt Generator with Multi-Model Support

Generates intelligent, context-aware prompts optimized for different AI models.
Supports GPT-4, Claude, Gemini, and other modern LLMs with model-specific optimizations.
"""

from typing import Dict, List, Any, Optional, Union
from datetime import datetime
from pathlib import Path
import json
from enum import Enum

from ..analyzers.project_intelligence import ProjectAnalysisResult, ProjectIssue

class AIModel(Enum):
    """Supported AI models with their characteristics."""
    GPT_4 = "gpt-4"
    GPT_4_TURBO = "gpt-4-turbo"
    CLAUDE_SONNET = "claude-3-sonnet"
    CLAUDE_OPUS = "claude-3-opus"
    CLAUDE_HAIKU = "claude-3-haiku"
    GEMINI_PRO = "gemini-pro"
    GEMINI_ULTRA = "gemini-ultra"
    CODELLAMA = "codellama"
    MIXTRAL = "mixtral"

class PromptTemplate:
    """Template for model-specific prompts."""
    def __init__(self, model: AIModel, max_tokens: int, temperature: float, system_prompt: str = ""):
        self.model = model
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.system_prompt = system_prompt

class SmartPromptGenerator:
    """
    Generates intelligent, context-aware prompts optimized for different AI models.
    """
    
    def __init__(self, analysis_result: ProjectAnalysisResult, context_data: Dict[str, Any] = None, 
                 target_model: AIModel = AIModel.GPT_4):
        self.analysis = analysis_result
        self.context = context_data or {}
        self.project_name = Path(analysis_result.project_path).name
        self.target_model = target_model
        self.model_templates = self._initialize_model_templates()
        self.prompt_library = self._load_prompt_library()
    
    def generate_critical_issues_prompt(self) -> str:
        """Generate prompt to fix all critical issues."""
        if not self.analysis.critical_issues:
            return self._generate_no_critical_issues_prompt()
        
        issues_by_type = self._group_issues_by_type(self.analysis.critical_issues)
        
        prompt = f"""# Fix Critical Issues in {self.project_name}

## Project Health Analysis
**Health Score**: {self.analysis.health_score}/100
**Analysis Date**: {datetime.fromisoformat(self.analysis.analysis_timestamp).strftime('%Y-%m-%d %H:%M')}
**Technology Stack**: {', '.join(self.analysis.tech_stack)}

## ðŸš¨ Critical Issues Identified

{self._format_issues_for_prompt(self.analysis.critical_issues)}

## Priority Action Plan

### Immediate Actions Required:
{self._generate_action_plan(self.analysis.critical_issues)}

### Implementation Strategy:
1. **Start with blocking issues** - Focus on empty files and compilation errors first
2. **Address security vulnerabilities** - Fix any hardcoded secrets or security flaws
3. **Complete core functionality** - Implement stub components and missing features
4. **Validate fixes** - Test each fix thoroughly before moving to next issue

### Project Context for Implementation:
- **Architecture**: {self.analysis.project_type.title()} project with {self._get_file_count()} files
- **Key Technologies**: {', '.join(self.analysis.tech_stack)}
- **Existing Patterns**: Follow the established patterns in the codebase for consistency

## Requirements for Each Fix:
1. **Maintain existing architecture** and coding patterns
2. **Include proper error handling** and validation
3. **Add appropriate logging** where needed
4. **Write or update tests** for new functionality
5. **Follow the project's TypeScript/coding standards**
6. **Ensure backward compatibility** with existing features

---
*Generated by Prompt Engineer - Intelligent Analysis on {datetime.now().strftime('%Y-%m-%d %H:%M')}*
"""
        return prompt
    
    def generate_specific_issue_prompt(self, issue: ProjectIssue) -> str:
        """Generate a specific prompt for a single issue."""
        context = self._get_issue_context(issue)
        
        prompt = f"""# {issue.title} - {self.project_name}

## Issue Details
**Type**: {issue.type.replace('_', ' ').title()}
**Severity**: {issue.severity.upper()}
**Description**: {issue.description}
{f"**File**: {issue.file_path}" if issue.file_path else ""}
{f"**Line**: {issue.line_number}" if issue.line_number else ""}

## Project Context
**Project**: {self.project_name} ({self.analysis.project_type} project)
**Technology Stack**: {', '.join(self.analysis.tech_stack)}
**Health Score**: {self.analysis.health_score}/100

{context}

## Specific Request
{self._generate_specific_action_request(issue)}

## Implementation Requirements
{self._generate_implementation_requirements(issue)}

## Acceptance Criteria
{self._generate_acceptance_criteria(issue)}

---
*Generated by Prompt Engineer - Intelligent Analysis on {datetime.now().strftime('%Y-%m-%d %H:%M')}*
"""
        return prompt
    
    def generate_missing_features_prompt(self) -> str:
        """Generate prompt to add commonly missing features."""
        missing_features = [i for i in self.analysis.medium_priority_issues + self.analysis.low_priority_issues 
                           if i.type == 'missing_feature']
        
        if not missing_features:
            return f"""# Feature Enhancement Opportunities - {self.project_name}

## Current Status
Your **{self.project_name}** project appears to have good feature coverage! 

**Health Score**: {self.analysis.health_score}/100
**Project Type**: {self.analysis.project_type}
**Technology Stack**: {', '.join(self.analysis.tech_stack)}

## Recommendations for Further Enhancement
Based on analysis of similar {self.analysis.project_type} projects, consider these advanced features:

{self._generate_advanced_features_suggestions()}

## Next Steps
1. **Review current functionality** against user needs
2. **Gather user feedback** on most-wanted features  
3. **Prioritize features** based on business value and complexity
4. **Plan implementation** in iterative sprints

---
*No critical missing features detected - you're on the right track!*
"""
        
        prompt = f"""# Add Missing Features to {self.project_name}

## Project Analysis
**Health Score**: {self.analysis.health_score}/100
**Project Type**: {self.analysis.project_type}
**Technology Stack**: {', '.join(self.analysis.tech_stack)}

## ðŸ” Missing Features Detected

{self._format_missing_features(missing_features)}

## Implementation Plan

### Phase 1: Core Functionality
{self._prioritize_missing_features(missing_features, 'core')}

### Phase 2: User Experience
{self._prioritize_missing_features(missing_features, 'ux')}

### Phase 3: Quality & Security
{self._prioritize_missing_features(missing_features, 'quality')}

## Implementation Guidelines
1. **Follow existing patterns** in the codebase
2. **Start with highest-impact features** (error handling, testing)
3. **Implement incrementally** to avoid breaking changes
4. **Test each feature** thoroughly before moving to the next
5. **Update documentation** as you add features

## Technical Requirements
- **Maintain compatibility** with current {', '.join(self.analysis.tech_stack)} architecture
- **Use established libraries** and patterns from the existing codebase
- **Include comprehensive error handling** and user feedback
- **Add appropriate tests** for all new functionality

---
*Generated by Prompt Engineer - Intelligent Analysis on {datetime.now().strftime('%Y-%m-%d %H:%M')}*
"""
        return prompt
    
    def generate_comprehensive_improvement_prompt(self) -> str:
        """Generate a comprehensive prompt covering all improvements."""
        total_issues = (len(self.analysis.critical_issues) + len(self.analysis.high_priority_issues) + 
                       len(self.analysis.medium_priority_issues))
        
        prompt = f"""# Comprehensive Improvement Plan - {self.project_name}

## Executive Summary
**Project Health**: {self.analysis.health_score}/100
**Total Issues Found**: {total_issues}
**Technology Stack**: {', '.join(self.analysis.tech_stack)}
**Analysis Date**: {datetime.fromisoformat(self.analysis.analysis_timestamp).strftime('%Y-%m-%d %H:%M')}

## ðŸ“Š Issue Breakdown
- ðŸš¨ **Critical**: {len(self.analysis.critical_issues)} issues
- âš ï¸ **High Priority**: {len(self.analysis.high_priority_issues)} issues  
- ðŸ“‹ **Medium Priority**: {len(self.analysis.medium_priority_issues)} issues
- ðŸ’¡ **Low Priority**: {len(self.analysis.low_priority_issues)} issues

## ðŸŽ¯ Strategic Recommendations
{chr(10).join(f'- {suggestion}' for suggestion in self.analysis.suggestions)}

## Phase 1: Critical Issues (Start Here)
{self._format_issues_for_prompt(self.analysis.critical_issues) if self.analysis.critical_issues else "âœ… No critical issues found!"}

## Phase 2: High Priority Issues  
{self._format_issues_for_prompt(self.analysis.high_priority_issues) if self.analysis.high_priority_issues else "âœ… No high priority issues found!"}

## Phase 3: Quality Improvements
{self._format_issues_for_prompt(self.analysis.medium_priority_issues[:5]) if self.analysis.medium_priority_issues else "âœ… Code quality looks good!"}

## Implementation Strategy

### Week 1: Stability & Critical Fixes
- Address all critical issues first
- Fix empty/stub files
- Resolve security vulnerabilities
- Ensure basic functionality works

### Week 2: Feature Completeness
- Implement missing core features
- Add proper error handling
- Complete test coverage gaps
- Address high-priority TODOs

### Week 3: Polish & Quality
- Code quality improvements
- Documentation updates
- Performance optimizations
- User experience enhancements

## Success Metrics
- [ ] Health score improved to 90+
- [ ] All critical and high-priority issues resolved
- [ ] Core functionality fully implemented
- [ ] Comprehensive test coverage
- [ ] Security vulnerabilities addressed
- [ ] Documentation updated and complete

## Technical Guidelines
1. **Maintain existing architecture** - Don't break current functionality
2. **Follow established patterns** - Stay consistent with existing code style
3. **Test incrementally** - Validate each fix before moving on
4. **Document changes** - Update README and inline documentation
5. **Security first** - Address all security issues before feature work

---
*This comprehensive plan will transform your project from {self.analysis.health_score}/100 to a production-ready codebase.*
"""
        return prompt
    
    def generate_test_improvement_prompt(self) -> str:
        """Generate prompt specifically for improving tests."""
        test_issues = [i for i in (self.analysis.critical_issues + self.analysis.high_priority_issues + 
                                  self.analysis.medium_priority_issues) if 'test' in i.type.lower()]
        
        prompt = f"""# Improve Testing Strategy - {self.project_name}

## Current Testing Status
**Project**: {self.project_name} ({self.analysis.project_type})
**Technology Stack**: {', '.join(self.analysis.tech_stack)}
**Health Score**: {self.analysis.health_score}/100

## ðŸ§ª Testing Issues Identified
{self._format_test_issues(test_issues) if test_issues else "No specific testing issues detected, but let's enhance your testing strategy."}

## Recommended Testing Strategy

### For {self.analysis.project_type.title()} Projects:
{self._get_testing_recommendations_by_type(self.analysis.project_type)}

### Implementation Plan:
1. **Set up testing framework** (if not already present)
2. **Write unit tests** for core business logic
3. **Add integration tests** for critical user flows
4. **Create component tests** (for frontend projects)
5. **Set up automated testing** in CI/CD pipeline

### Priority Test Areas:
{self._identify_priority_test_areas()}

## Testing Best Practices for Your Stack
{self._generate_stack_specific_testing_guidance()}

## Implementation Steps
1. **Assessment**: Review current test coverage
2. **Framework Setup**: Ensure proper testing tools are configured
3. **Core Tests**: Start with business logic and critical functions
4. **Integration**: Test component interactions and API calls
5. **E2E**: Add end-to-end tests for key user journeys
6. **Automation**: Set up continuous testing in your development workflow

---
*A robust testing strategy will significantly improve your project's reliability and maintainability.*
"""
        return prompt
    
    def _group_issues_by_type(self, issues: List[ProjectIssue]) -> Dict[str, List[ProjectIssue]]:
        """Group issues by their type."""
        groups = {}
        for issue in issues:
            if issue.type not in groups:
                groups[issue.type] = []
            groups[issue.type].append(issue)
        return groups
    
    def _format_issues_for_prompt(self, issues: List[ProjectIssue]) -> str:
        """Format issues for inclusion in prompts."""
        if not issues:
            return "No issues found in this category."
        
        formatted = []
        for i, issue in enumerate(issues, 1):
            location = ""
            if issue.file_path:
                location = f" in `{issue.file_path}`"
                if issue.line_number:
                    location += f" (line {issue.line_number})"
            
            formatted.append(f"""
### {i}. {issue.title}
**Description**: {issue.description}{location}
**Action**: {issue.suggested_action or 'Address this issue'}
""")
        
        return "\n".join(formatted)
    
    def _generate_action_plan(self, issues: List[ProjectIssue]) -> str:
        """Generate specific action plan for issues."""
        if not issues:
            return "No immediate actions required."
        
        actions = []
        for i, issue in enumerate(issues, 1):
            action = issue.suggested_action or f"Address {issue.title.lower()}"
            location = f" ({issue.file_path})" if issue.file_path else ""
            actions.append(f"{i}. **{action}**{location}")
        
        return "\n".join(actions)
    
    def _get_issue_context(self, issue: ProjectIssue) -> str:
        """Get additional context for a specific issue."""
        context_parts = []
        
        if issue.context:
            for key, value in issue.context.items():
                if key == 'file_size':
                    context_parts.append(f"**File Size**: {value} bytes")
                elif key == 'content_preview':
                    context_parts.append(f"**Content Preview**: `{value}`")
        
        if context_parts:
            return "## Additional Context\n" + "\n".join(context_parts) + "\n"
        
        return ""
    
    def _generate_specific_action_request(self, issue: ProjectIssue) -> str:
        """Generate specific action request based on issue type."""
        if issue.type == 'empty_file':
            return f"""Please implement the **{Path(issue.file_path).stem}** component/module that is currently empty or stub.
            
**Specific requirements:**
- Follow the existing code patterns in the project
- Implement the full functionality this file is meant to provide
- Add proper TypeScript types and interfaces
- Include error handling and validation
- Match the coding style of similar files in the project"""
        
        elif issue.type == 'todo':
            return f"""Please address the {issue.title.split()[0]} comment: "{issue.description}"
            
**Specific requirements:**
- Implement the functionality described in the comment
- Remove or update the comment once resolved
- Ensure the implementation follows project conventions
- Add tests if the change affects functionality"""
        
        elif issue.type == 'missing_feature':
            return f"""Please implement {issue.title} functionality in the project.
            
**Specific requirements:**
- Research best practices for {issue.title} in {self.analysis.project_type} projects
- Implement following the established patterns in the codebase
- Ensure integration with existing functionality
- Add appropriate tests and documentation"""
        
        elif issue.type == 'security':
            return f"""Please fix the security vulnerability: {issue.description}
            
**Specific requirements:**
- Remove or properly secure the identified vulnerability
- Follow security best practices for {', '.join(self.analysis.tech_stack)}
- Ensure the fix doesn't break existing functionality
- Consider adding security tests to prevent regression"""
        
        else:
            return f"""Please address this issue: {issue.description}
            
**Specific requirements:**
- Follow the project's existing patterns and conventions
- Ensure the fix is complete and thoroughly tested
- Update documentation if necessary"""
    
    def _generate_implementation_requirements(self, issue: ProjectIssue) -> str:
        """Generate implementation requirements for the issue."""
        base_requirements = f"""
1. **Architecture Compatibility**: Ensure changes work with existing {self.analysis.project_type} architecture
2. **Technology Stack**: Use the established {', '.join(self.analysis.tech_stack)} patterns
3. **Code Quality**: Follow the project's coding standards and patterns
4. **Testing**: Add or update tests as appropriate
5. **Documentation**: Update inline documentation and comments"""
        
        if issue.type == 'empty_file' and '.tsx' in (issue.file_path or ''):
            base_requirements += """
6. **React Patterns**: Follow existing component patterns and hooks usage
7. **TypeScript**: Use proper typing and interfaces
8. **Styling**: Match existing styling approach and theme"""
        
        return base_requirements
    
    def _generate_acceptance_criteria(self, issue: ProjectIssue) -> str:
        """Generate acceptance criteria for the issue."""
        criteria = [
            "âœ… Issue is completely resolved",
            "âœ… No new bugs or regressions introduced", 
            "âœ… Code follows project conventions and patterns",
            "âœ… Changes are properly tested"
        ]
        
        if issue.type == 'empty_file':
            criteria.extend([
                "âœ… File implements the intended functionality",
                "âœ… Component integrates properly with the application",
                "âœ… Proper error handling is included"
            ])
        
        elif issue.type == 'todo':
            criteria.extend([
                "âœ… TODO comment is addressed and removed/updated",
                "âœ… Implementation matches the intent of the comment"
            ])
        
        elif issue.type == 'security':
            criteria.extend([
                "âœ… Security vulnerability is completely fixed",
                "âœ… No similar vulnerabilities remain in the codebase"
            ])
        
        return "\n".join(criteria)
    
    def _get_file_count(self) -> str:
        """Get approximate file count from context."""
        if self.context and 'code_structure' in self.context:
            file_count = self.context['code_structure'].get('file_count', 'multiple')
            return f"{file_count}" if isinstance(file_count, int) else "multiple"
        return "multiple"
    
    def _generate_no_critical_issues_prompt(self) -> str:
        """Generate prompt when no critical issues are found."""
        return f"""# Excellent! No Critical Issues Found - {self.project_name}

## Project Health Report
**Health Score**: {self.analysis.health_score}/100 âœ…
**Status**: Your project is in good health!
**Technology Stack**: {', '.join(self.analysis.tech_stack)}

## Current Status Summary
- ðŸš¨ **Critical Issues**: 0 (Excellent!)
- âš ï¸ **High Priority**: {len(self.analysis.high_priority_issues)} issues
- ðŸ“‹ **Medium Priority**: {len(self.analysis.medium_priority_issues)} issues

## Recommendations for Continued Excellence
{chr(10).join(f'- {suggestion}' for suggestion in self.analysis.suggestions)}

## Optional Improvements
{self._format_issues_for_prompt(self.analysis.high_priority_issues[:3]) if self.analysis.high_priority_issues else "Consider adding advanced features or performance optimizations."}

## Keep Up the Great Work!
Your {self.project_name} project demonstrates good development practices. Focus on the high-priority items when you have time, but there's nothing blocking or critical that needs immediate attention.

---
*Project health analysis complete - you're doing great!*
"""

    def _format_missing_features(self, missing_features: List[ProjectIssue]) -> str:
        """Format missing features for prompt."""
        if not missing_features:
            return "No missing features detected - your project has good feature coverage!"
        
        formatted = []
        for i, feature in enumerate(missing_features, 1):
            formatted.append(f"""
### {i}. {feature.title}
**Impact**: {feature.description}
**Implementation**: {feature.suggested_action}
""")
        
        return "\n".join(formatted)
    
    def _prioritize_missing_features(self, features: List[ProjectIssue], phase: str) -> str:
        """Prioritize missing features by implementation phase."""
        priorities = {
            'core': ['Error Handling', 'Logging', 'Authentication', 'Input Validation'],
            'ux': ['Loading States', 'Error Boundaries', 'User Feedback'],
            'quality': ['Testing', 'Documentation', 'Type Hints', 'Security Headers']
        }
        
        phase_features = []
        for feature in features:
            for priority_feature in priorities.get(phase, []):
                if priority_feature.lower() in feature.title.lower():
                    phase_features.append(f"- **{feature.title}**: {feature.suggested_action}")
        
        if not phase_features:
            return f"- No {phase} features identified for this phase"
        
        return "\n".join(phase_features)
    
    def _generate_advanced_features_suggestions(self) -> str:
        """Generate advanced feature suggestions for healthy projects."""
        suggestions = {
            'react': [
                "ðŸš€ **Progressive Web App (PWA)** capabilities for offline usage",
                "ðŸ“Š **Advanced Analytics** and user behavior tracking", 
                "ðŸŽ¨ **Theme System** with dark/light mode support",
                "ðŸ”„ **Real-time Features** with WebSocket integration",
                "ðŸ“± **Mobile Optimization** and responsive design enhancements"
            ],
            'python': [
                "âš¡ **Performance Optimization** with async/await patterns",
                "ðŸ³ **Containerization** with Docker for easy deployment",
                "ðŸ“ˆ **Monitoring & Metrics** with detailed logging and alerts",
                "ðŸ”Œ **Plugin Architecture** for extensibility",
                "ðŸ”’ **Advanced Security** with OAuth and rate limiting"
            ]
        }
        
        project_suggestions = suggestions.get(self.analysis.project_type, [
            "ðŸš€ Performance optimizations and monitoring",
            "ðŸ”’ Enhanced security features",
            "ðŸ“Š Advanced analytics and reporting",
            "ðŸŽ¨ User experience improvements",
            "ðŸ”§ Developer experience enhancements"
        ])
        
        return "\n".join(project_suggestions)
    
    def _format_test_issues(self, test_issues: List[ProjectIssue]) -> str:
        """Format test-related issues."""
        if not test_issues:
            return ""
        
        formatted = []
        for issue in test_issues:
            formatted.append(f"- **{issue.title}**: {issue.description}")
        
        return "\n".join(formatted)
    
    def _get_testing_recommendations_by_type(self, project_type: str) -> str:
        """Get testing recommendations based on project type."""
        recommendations = {
            'react': """
**Unit Tests**: Test individual components with Jest and React Testing Library
**Integration Tests**: Test component interactions and API calls  
**E2E Tests**: Use Cypress or Playwright for full user journey testing
**Visual Tests**: Consider Storybook for component documentation and testing""",
            'python': """
**Unit Tests**: Use pytest for function and class testing
**Integration Tests**: Test database interactions and external APIs
**Property Tests**: Consider hypothesis for property-based testing
**Performance Tests**: Add benchmarking for critical algorithms""",
            'node': """
**Unit Tests**: Test individual functions and modules with Jest
**API Tests**: Test endpoints with supertest or similar tools
**Integration Tests**: Test database and external service interactions
**Load Tests**: Consider performance testing for high-traffic endpoints"""
        }
        
        return recommendations.get(project_type, """
**Unit Tests**: Test individual functions and components
**Integration Tests**: Test component interactions
**End-to-End Tests**: Test complete user workflows
**Performance Tests**: Monitor and test critical performance metrics""")
    
    def _identify_priority_test_areas(self) -> str:
        """Identify priority areas for testing based on project analysis."""
        areas = []
        
        # Check for critical components that need testing
        empty_files = [i for i in self.analysis.critical_issues + self.analysis.high_priority_issues 
                      if i.type == 'empty_file']
        if empty_files:
            areas.append("ðŸš¨ **Newly implemented components** (test empty files after implementation)")
        
        # Core business logic
        areas.append("ðŸ’¼ **Core business logic** and data processing functions")
        areas.append("ðŸ”Œ **API integrations** and external service interactions")
        areas.append("ðŸŽ¯ **User authentication** and authorization flows")
        areas.append("ðŸ›¡ï¸ **Input validation** and error handling")
        
        return "\n".join(areas)
    
    def _generate_stack_specific_testing_guidance(self) -> str:
        """Generate testing guidance specific to the technology stack."""
        if 'React' in self.analysis.tech_stack:
            return """
**React Testing Best Practices:**
- Use React Testing Library for component testing
- Test user interactions, not implementation details
- Mock external dependencies and API calls
- Test accessibility with screen readers in mind
- Use MSW (Mock Service Worker) for API mocking"""
        
        elif 'Python' in self.analysis.tech_stack:
            return """
**Python Testing Best Practices:**
- Use pytest with fixtures for test data
- Mock external dependencies with unittest.mock
- Test edge cases and error conditions
- Use parametrized tests for multiple inputs
- Include docstring examples that can be tested with doctest"""
        
        else:
            return """
**General Testing Best Practices:**
- Write tests that are maintainable and readable
- Test behavior, not implementation details
- Mock external dependencies to isolate units
- Include both positive and negative test cases
- Keep tests fast and independent of each other"""
    
    def _initialize_model_templates(self) -> Dict[AIModel, PromptTemplate]:
        """Initialize model-specific prompt templates."""
        return {
            AIModel.GPT_4: PromptTemplate(
                model=AIModel.GPT_4,
                max_tokens=4000,
                temperature=0.7,
                system_prompt="You are an expert software engineer and architect. Provide detailed, actionable solutions."
            ),
            AIModel.GPT_4_TURBO: PromptTemplate(
                model=AIModel.GPT_4_TURBO,
                max_tokens=8000,
                temperature=0.7,
                system_prompt="You are an expert software engineer. Focus on modern best practices and efficient solutions."
            ),
            AIModel.CLAUDE_SONNET: PromptTemplate(
                model=AIModel.CLAUDE_SONNET,
                max_tokens=4000,
                temperature=0.7,
                system_prompt="You are a thoughtful software engineer. Provide well-reasoned, step-by-step solutions."
            ),
            AIModel.CLAUDE_OPUS: PromptTemplate(
                model=AIModel.CLAUDE_OPUS,
                max_tokens=4000,
                temperature=0.7,
                system_prompt="You are an expert software architect. Provide comprehensive analysis and solutions."
            ),
            AIModel.CLAUDE_HAIKU: PromptTemplate(
                model=AIModel.CLAUDE_HAIKU,
                max_tokens=2000,
                temperature=0.7,
                system_prompt="You are a practical software engineer. Provide concise, actionable solutions."
            ),
            AIModel.GEMINI_PRO: PromptTemplate(
                model=AIModel.GEMINI_PRO,
                max_tokens=4000,
                temperature=0.7,
                system_prompt="You are a knowledgeable software developer. Focus on practical implementations."
            ),
            AIModel.GEMINI_ULTRA: PromptTemplate(
                model=AIModel.GEMINI_ULTRA,
                max_tokens=8000,
                temperature=0.7,
                system_prompt="You are an expert software architect. Provide detailed analysis and comprehensive solutions."
            ),
            AIModel.CODELLAMA: PromptTemplate(
                model=AIModel.CODELLAMA,
                max_tokens=4000,
                temperature=0.5,
                system_prompt="You are a code-focused AI assistant. Provide clean, well-documented code solutions."
            ),
            AIModel.MIXTRAL: PromptTemplate(
                model=AIModel.MIXTRAL,
                max_tokens=4000,
                temperature=0.7,
                system_prompt="You are an experienced software engineer. Provide balanced, practical solutions."
            )
        }
    
    def _load_prompt_library(self) -> Dict[str, Dict[str, str]]:
        """Load library of 60+ specialized prompts for different scenarios."""
        return {
            "code_review": {
                "title": "Comprehensive Code Review",
                "description": "Review code for quality, security, and best practices",
                "template": """
# Code Review Request

## Code to Review
{code_content}

## Review Criteria
- Code quality and readability
- Security vulnerabilities
- Performance considerations
- Best practices adherence
- Test coverage

## Project Context
- Technology Stack: {tech_stack}
- Project Type: {project_type}
- Architecture Patterns: {architecture_patterns}

Please provide:
1. Overall assessment
2. Specific issues found
3. Recommended improvements
4. Priority ranking of issues
"""
            },
            "bug_fixing": {
                "title": "Bug Analysis and Fix",
                "description": "Analyze and fix specific bugs in code",
                "template": """
# Bug Fix Request

## Bug Description
{bug_description}

## Code Context
{code_content}

## Expected Behavior
{expected_behavior}

## Current Behavior
{current_behavior}

## Technical Context
- Technology Stack: {tech_stack}
- File Path: {file_path}
- Related Components: {related_components}

Please provide:
1. Root cause analysis
2. Specific fix implementation
3. Test cases to verify fix
4. Prevention strategies
"""
            },
            "feature_implementation": {
                "title": "Feature Implementation Plan",
                "description": "Plan and implement new features",
                "template": """
# Feature Implementation Request

## Feature Specification
{feature_spec}

## Requirements
{requirements}

## Acceptance Criteria
{acceptance_criteria}

## Technical Context
- Current Architecture: {architecture}
- Technology Stack: {tech_stack}
- Integration Points: {integration_points}

## Constraints
{constraints}

Please provide:
1. Implementation approach
2. Code structure and files needed
3. Database changes (if any)
4. Testing strategy
5. Deployment considerations
"""
            },
            "refactoring": {
                "title": "Code Refactoring Plan",
                "description": "Refactor code for better quality and maintainability",
                "template": """
# Code Refactoring Request

## Current Code
{current_code}

## Issues Identified
{issues}

## Refactoring Goals
{goals}

## Technical Context
- Technology Stack: {tech_stack}
- Current Architecture: {architecture}
- Constraints: {constraints}

Please provide:
1. Refactoring strategy
2. Step-by-step plan
3. Risk assessment
4. Testing approach
5. Migration strategy (if needed)
"""
            },
            "architecture_design": {
                "title": "Architecture Design",
                "description": "Design system architecture for scalability and maintainability",
                "template": """
# Architecture Design Request

## System Requirements
{requirements}

## Scale and Performance Needs
{scale_requirements}

## Technical Constraints
{constraints}

## Current Technology Stack
{tech_stack}

## Integration Requirements
{integrations}

Please provide:
1. Recommended architecture pattern
2. Component design and interactions
3. Data flow and storage strategy
4. Scalability considerations
5. Technology recommendations
6. Implementation roadmap
"""
            },
            "performance_optimization": {
                "title": "Performance Optimization",
                "description": "Optimize code and system performance",
                "template": """
# Performance Optimization Request

## Current Performance Issues
{performance_issues}

## Code/System Context
{code_content}

## Performance Metrics
- Current performance: {current_metrics}
- Target performance: {target_metrics}

## Technical Context
- Technology Stack: {tech_stack}
- Architecture: {architecture}
- Scale: {scale_info}

Please provide:
1. Performance analysis
2. Bottleneck identification
3. Optimization strategies
4. Implementation plan
5. Monitoring recommendations
"""
            },
            "security_audit": {
                "title": "Security Audit",
                "description": "Audit code for security vulnerabilities",
                "template": """
# Security Audit Request

## Code to Audit
{code_content}

## Application Context
- Type: {app_type}
- Technology Stack: {tech_stack}
- User Data Handling: {data_handling}
- External Integrations: {integrations}

## Security Requirements
{security_requirements}

Please provide:
1. Vulnerability assessment
2. Risk analysis
3. Security recommendations
4. Implementation priorities
5. Compliance considerations
"""
            },
            "api_design": {
                "title": "API Design",
                "description": "Design RESTful APIs and GraphQL schemas",
                "template": """
# API Design Request

## API Requirements
{api_requirements}

## Data Models
{data_models}

## Use Cases
{use_cases}

## Technical Context
- Technology Stack: {tech_stack}
- Authentication: {auth_method}
- Expected Scale: {scale}

Please provide:
1. API endpoint design
2. Request/Response schemas
3. Authentication/authorization strategy
4. Error handling approach
5. Documentation template
6. Testing strategy
"""
            },
            "database_design": {
                "title": "Database Design",
                "description": "Design database schemas and queries",
                "template": """
# Database Design Request

## Data Requirements
{data_requirements}

## Use Cases
{use_cases}

## Performance Requirements
{performance_requirements}

## Technical Context
- Database Type: {db_type}
- Expected Scale: {scale}
- Technology Stack: {tech_stack}

Please provide:
1. Database schema design
2. Relationship modeling
3. Query optimization strategies
4. Indexing recommendations
5. Migration plan
"""
            },
            "testing_strategy": {
                "title": "Testing Strategy",
                "description": "Design comprehensive testing approaches",
                "template": """
# Testing Strategy Request

## Code/Feature to Test
{code_content}

## Testing Requirements
{testing_requirements}

## Technical Context
- Technology Stack: {tech_stack}
- Testing Framework: {test_framework}
- CI/CD Pipeline: {cicd_info}

Please provide:
1. Testing approach (unit/integration/e2e)
2. Test case design
3. Mock strategy
4. Test automation recommendations
5. Coverage goals and metrics
"""
            }
        }
    
    def generate_model_optimized_prompt(self, prompt_type: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Generate prompt optimized for the target model."""
        if prompt_type not in self.prompt_library:
            raise ValueError(f"Unknown prompt type: {prompt_type}")
        
        template_info = self.prompt_library[prompt_type]
        model_template = self.model_templates[self.target_model]
        
        # Format the prompt template with context
        formatted_prompt = template_info["template"].format(**context)
        
        # Apply model-specific optimizations
        optimized_prompt = self._apply_model_optimizations(formatted_prompt, self.target_model)
        
        return {
            "prompt": optimized_prompt,
            "system_prompt": model_template.system_prompt,
            "model": self.target_model.value,
            "max_tokens": model_template.max_tokens,
            "temperature": model_template.temperature,
            "metadata": {
                "prompt_type": prompt_type,
                "title": template_info["title"],
                "description": template_info["description"],
                "generated_at": datetime.now().isoformat()
            }
        }
    
    def _apply_model_optimizations(self, prompt: str, model: AIModel) -> str:
        """Apply model-specific optimizations to prompts."""
        optimized = prompt
        
        if model in [AIModel.CLAUDE_SONNET, AIModel.CLAUDE_OPUS, AIModel.CLAUDE_HAIKU]:
            # Claude models prefer structured thinking
            optimized = self._add_thinking_structure(optimized)
            # Claude responds well to explicit instructions
            optimized = self._make_instructions_explicit(optimized)
            
        elif model in [AIModel.GPT_4, AIModel.GPT_4_TURBO]:
            # GPT-4 models benefit from clear roles and examples
            optimized = self._add_role_clarity(optimized)
            optimized = self._add_output_format_examples(optimized)
            
        elif model in [AIModel.GEMINI_PRO, AIModel.GEMINI_ULTRA]:
            # Gemini models work well with step-by-step instructions
            optimized = self._add_step_by_step_structure(optimized)
            
        elif model == AIModel.CODELLAMA:
            # CodeLlama focuses on code, minimize prose
            optimized = self._minimize_prose_maximize_code(optimized)
            
        elif model == AIModel.MIXTRAL:
            # Mixtral benefits from clear context and examples
            optimized = self._add_context_examples(optimized)
        
        return optimized
    
    def _add_thinking_structure(self, prompt: str) -> str:
        """Add thinking structure for Claude models."""
        thinking_section = """
## Analysis Approach
Please think through this systematically:
1. **Understanding**: First, understand the requirements and context
2. **Analysis**: Analyze the current situation and identify key issues
3. **Solution Design**: Design the optimal approach
4. **Implementation**: Provide specific implementation details
5. **Validation**: Suggest how to verify the solution works

"""
        return prompt + thinking_section
    
    def _make_instructions_explicit(self, prompt: str) -> str:
        """Make instructions more explicit for Claude."""
        explicit_section = """
## Output Requirements
Please provide your response with:
- Clear section headers
- Specific, actionable recommendations
- Code examples where applicable
- Reasoning for your decisions
- Next steps for implementation

"""
        return prompt + explicit_section
    
    def _add_role_clarity(self, prompt: str) -> str:
        """Add role clarity for GPT models."""
        role_section = """
## Your Role
Act as an expert software engineer with deep knowledge of:
- Modern development practices
- Architecture patterns
- Code quality standards
- Performance optimization
- Security best practices

"""
        return role_section + prompt
    
    def _add_output_format_examples(self, prompt: str) -> str:
        """Add output format examples for GPT models."""
        format_section = """
## Expected Output Format
Structure your response like this:

### Analysis
[Your analysis of the situation]

### Recommendations
1. [Specific recommendation with reasoning]
2. [Another recommendation]

### Implementation
```language
// Code example
```

### Next Steps
- [ ] Action item 1
- [ ] Action item 2

"""
        return prompt + format_section
    
    def _add_step_by_step_structure(self, prompt: str) -> str:
        """Add step-by-step structure for Gemini models."""
        steps_section = """
## Step-by-Step Approach
Please work through this systematically:

**Step 1: Requirements Analysis**
- Review and understand all requirements
- Identify constraints and dependencies

**Step 2: Solution Design**
- Design the optimal approach
- Consider alternatives and trade-offs

**Step 3: Implementation Planning**
- Break down into implementable tasks
- Identify required resources and tools

**Step 4: Quality Assurance**
- Plan testing strategy
- Consider error handling and edge cases

**Step 5: Documentation**
- Document the solution
- Provide usage examples

"""
        return prompt + steps_section
    
    def _minimize_prose_maximize_code(self, prompt: str) -> str:
        """Minimize prose for CodeLlama models."""
        code_focused = """
## Code-Focused Response Needed
Provide:
- Minimal explanatory text
- Maximum code examples
- Clear code comments
- Working implementations
- Test cases

Focus on practical, executable solutions.

"""
        return prompt + code_focused
    
    def _add_context_examples(self, prompt: str) -> str:
        """Add context examples for Mixtral."""
        context_section = """
## Context and Examples
When providing solutions, please include:
- Real-world examples
- Common use cases
- Best practices from the industry
- Lessons learned from similar projects

"""
        return prompt + context_section
    
    def get_available_prompt_types(self) -> List[Dict[str, str]]:
        """Get list of available prompt types."""
        return [
            {
                "type": key,
                "title": value["title"],
                "description": value["description"]
            }
            for key, value in self.prompt_library.items()
        ]
    
    def switch_model(self, new_model: AIModel):
        """Switch the target AI model."""
        self.target_model = new_model
    
    def get_model_capabilities(self, model: AIModel) -> Dict[str, Any]:
        """Get capabilities information for a specific model."""
        capabilities = {
            AIModel.GPT_4: {
                "strengths": ["Complex reasoning", "Code generation", "Creative writing"],
                "best_for": ["Architecture design", "Complex problem solving"],
                "max_context": 8192,
                "supports_functions": True
            },
            AIModel.GPT_4_TURBO: {
                "strengths": ["Fast processing", "Large context", "Up-to-date knowledge"],
                "best_for": ["Large codebases", "Recent technologies"],
                "max_context": 128000,
                "supports_functions": True
            },
            AIModel.CLAUDE_OPUS: {
                "strengths": ["Thoughtful analysis", "Ethical reasoning", "Complex tasks"],
                "best_for": ["Architecture review", "Security analysis"],
                "max_context": 200000,
                "supports_functions": False
            },
            AIModel.CLAUDE_SONNET: {
                "strengths": ["Balanced performance", "Good reasoning", "Helpful responses"],
                "best_for": ["General development tasks", "Code review"],
                "max_context": 200000,
                "supports_functions": False
            },
            AIModel.CLAUDE_HAIKU: {
                "strengths": ["Fast responses", "Concise answers", "Cost-effective"],
                "best_for": ["Quick questions", "Simple tasks"],
                "max_context": 200000,
                "supports_functions": False
            },
            AIModel.GEMINI_PRO: {
                "strengths": ["Multimodal", "Good reasoning", "Google integration"],
                "best_for": ["Mixed content analysis", "Research tasks"],
                "max_context": 32000,
                "supports_functions": True
            },
            AIModel.GEMINI_ULTRA: {
                "strengths": ["Advanced reasoning", "Complex tasks", "High performance"],
                "best_for": ["Complex analysis", "Large projects"],
                "max_context": 32000,
                "supports_functions": True
            },
            AIModel.CODELLAMA: {
                "strengths": ["Code generation", "Code completion", "Programming focus"],
                "best_for": ["Pure coding tasks", "Code refactoring"],
                "max_context": 16000,
                "supports_functions": False
            },
            AIModel.MIXTRAL: {
                "strengths": ["Open source", "Good performance", "Balanced capabilities"],
                "best_for": ["General development", "Cost-conscious projects"],
                "max_context": 32000,
                "supports_functions": False
            }
        }
        
        return capabilities.get(model, {})
    
    def recommend_model_for_task(self, task_type: str, complexity: str = "medium") -> AIModel:
        """Recommend the best model for a specific task type."""
        recommendations = {
            "code_review": {
                "high": AIModel.CLAUDE_OPUS,
                "medium": AIModel.CLAUDE_SONNET,
                "low": AIModel.CLAUDE_HAIKU
            },
            "bug_fixing": {
                "high": AIModel.GPT_4,
                "medium": AIModel.GPT_4_TURBO,
                "low": AIModel.CODELLAMA
            },
            "architecture_design": {
                "high": AIModel.CLAUDE_OPUS,
                "medium": AIModel.GPT_4,
                "low": AIModel.GEMINI_PRO
            },
            "feature_implementation": {
                "high": AIModel.GPT_4_TURBO,
                "medium": AIModel.CLAUDE_SONNET,
                "low": AIModel.MIXTRAL
            },
            "performance_optimization": {
                "high": AIModel.GPT_4,
                "medium": AIModel.CLAUDE_SONNET,
                "low": AIModel.CODELLAMA
            },
            "security_audit": {
                "high": AIModel.CLAUDE_OPUS,
                "medium": AIModel.GPT_4,
                "low": AIModel.GEMINI_PRO
            },
            "testing_strategy": {
                "high": AIModel.CLAUDE_SONNET,
                "medium": AIModel.GPT_4_TURBO,
                "low": AIModel.MIXTRAL
            }
        }
        
        task_recommendations = recommendations.get(task_type, {})
        return task_recommendations.get(complexity, AIModel.GPT_4)