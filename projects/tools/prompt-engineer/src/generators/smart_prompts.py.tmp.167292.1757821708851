#!/usr/bin/env python3
"""
Enhanced Smart Prompt Generator with Multi-Model Support

Generates intelligent, context-aware prompts optimized for different AI models.
Supports GPT-4, Claude, Gemini, and other modern LLMs with model-specific optimizations.
"""

from typing import Dict, List, Any, Optional, Union
from datetime import datetime
from pathlib import Path
import json
from enum import Enum

from ..analyzers.project_intelligence import ProjectAnalysisResult, ProjectIssue

class AIModel(Enum):
    """Supported AI models with their characteristics."""
    GPT_4 = "gpt-4"
    GPT_4_TURBO = "gpt-4-turbo"
    CLAUDE_SONNET = "claude-3-sonnet"
    CLAUDE_OPUS = "claude-3-opus"
    CLAUDE_HAIKU = "claude-3-haiku"
    GEMINI_PRO = "gemini-pro"
    GEMINI_ULTRA = "gemini-ultra"
    CODELLAMA = "codellama"
    MIXTRAL = "mixtral"

class PromptTemplate:
    """Template for model-specific prompts."""
    def __init__(self, model: AIModel, max_tokens: int, temperature: float, system_prompt: str = ""):
        self.model = model
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.system_prompt = system_prompt

class SmartPromptGenerator:
    """
    Generates intelligent, context-aware prompts optimized for different AI models.
    """
    
    def __init__(self, analysis_result: ProjectAnalysisResult, context_data: Dict[str, Any] = None, 
                 target_model: AIModel = AIModel.GPT_4):
        self.analysis = analysis_result
        self.context = context_data or {}
        self.project_name = Path(analysis_result.project_path).name
        self.target_model = target_model
        self.model_templates = self._initialize_model_templates()
        self.prompt_library = self._load_prompt_library()
    
    def generate_critical_issues_prompt(self) -> str:
        """Generate prompt to fix all critical issues."""
        if not self.analysis.critical_issues:
            return self._generate_no_critical_issues_prompt()
        
        issues_by_type = self._group_issues_by_type(self.analysis.critical_issues)
        
        prompt = f"""# Fix Critical Issues in {self.project_name}

## Project Health Analysis
**Health Score**: {self.analysis.health_score}/100
**Analysis Date**: {datetime.fromisoformat(self.analysis.analysis_timestamp).strftime('%Y-%m-%d %H:%M')}
**Technology Stack**: {', '.join(self.analysis.tech_stack)}

## ðŸš¨ Critical Issues Identified

{self._format_issues_for_prompt(self.analysis.critical_issues)}

## Priority Action Plan

### Immediate Actions Required:
{self._generate_action_plan(self.analysis.critical_issues)}

### Implementation Strategy:
1. **Start with blocking issues** - Focus on empty files and compilation errors first
2. **Address security vulnerabilities** - Fix any hardcoded secrets or security flaws
3. **Complete core functionality** - Implement stub components and missing features
4. **Validate fixes** - Test each fix thoroughly before moving to next issue

### Project Context for Implementation:
- **Architecture**: {self.analysis.project_type.title()} project with {self._get_file_count()} files
- **Key Technologies**: {', '.join(self.analysis.tech_stack)}
- **Existing Patterns**: Follow the established patterns in the codebase for consistency

## Requirements for Each Fix:
1. **Maintain existing architecture** and coding patterns
2. **Include proper error handling** and validation
3. **Add appropriate logging** where needed
4. **Write or update tests** for new functionality
5. **Follow the project's TypeScript/coding standards**
6. **Ensure backward compatibility** with existing features

---
*Generated by Prompt Engineer - Intelligent Analysis on {datetime.now().strftime('%Y-%m-%d %H:%M')}*
"""
        return prompt
    
    def generate_specific_issue_prompt(self, issue: ProjectIssue) -> str:
        """Generate a specific prompt for a single issue."""
        context = self._get_issue_context(issue)
        
        prompt = f"""# {issue.title} - {self.project_name}

## Issue Details
**Type**: {issue.type.replace('_', ' ').title()}
**Severity**: {issue.severity.upper()}
**Description**: {issue.description}
{f"**File**: {issue.file_path}" if issue.file_path else ""}
{f"**Line**: {issue.line_number}" if issue.line_number else ""}

## Project Context
**Project**: {self.project_name} ({self.analysis.project_type} project)
**Technology Stack**: {', '.join(self.analysis.tech_stack)}
**Health Score**: {self.analysis.health_score}/100

{context}

## Specific Request
{self._generate_specific_action_request(issue)}

## Implementation Requirements
{self._generate_implementation_requirements(issue)}

## Acceptance Criteria
{self._generate_acceptance_criteria(issue)}

---
*Generated by Prompt Engineer - Intelligent Analysis on {datetime.now().strftime('%Y-%m-%d %H:%M')}*
"""
        return prompt
    
    def generate_missing_features_prompt(self) -> str:
        """Generate prompt to add commonly missing features."""
        missing_features = [i for i in self.analysis.medium_priority_issues + self.analysis.low_priority_issues 
                           if i.type == 'missing_feature']
        
        if not missing_features:
            return f"""# Feature Enhancement Opportunities - {self.project_name}

## Current Status
Your **{self.project_name}** project appears to have good feature coverage! 

**Health Score**: {self.analysis.health_score}/100
**Project Type**: {self.analysis.project_type}
**Technology Stack**: {', '.join(self.analysis.tech_stack)}

## Recommendations for Further Enhancement
Based on analysis of similar {self.analysis.project_type} projects, consider these advanced features:

{self._generate_advanced_features_suggestions()}

## Next Steps
1. **Review current functionality** against user needs
2. **Gather user feedback** on most-wanted features  
3. **Prioritize features** based on business value and complexity
4. **Plan implementation** in iterative sprints

---
*No critical missing features detected - you're on the right track!*
"""
        
        prompt = f"""# Add Missing Features to {self.project_name}

## Project Analysis
**Health Score**: {self.analysis.health_score}/100
**Project Type**: {self.analysis.project_type}
**Technology Stack**: {', '.join(self.analysis.tech_stack)}

## ðŸ” Missing Features Detected

{self._format_missing_features(missing_features)}

## Implementation Plan

### Phase 1: Core Functionality
{self._prioritize_missing_features(missing_features, 'core')}

### Phase 2: User Experience
{self._prioritize_missing_features(missing_features, 'ux')}

### Phase 3: Quality & Security
{self._prioritize_missing_features(missing_features, 'quality')}

## Implementation Guidelines
1. **Follow existing patterns** in the codebase
2. **Start with highest-impact features** (error handling, testing)
3. **Implement incrementally** to avoid breaking changes
4. **Test each feature** thoroughly before moving to the next
5. **Update documentation** as you add features

## Technical Requirements
- **Maintain compatibility** with current {', '.join(self.analysis.tech_stack)} architecture
- **Use established libraries** and patterns from the existing codebase
- **Include comprehensive error handling** and user feedback
- **Add appropriate tests** for all new functionality

---
*Generated by Prompt Engineer - Intelligent Analysis on {datetime.now().strftime('%Y-%m-%d %H:%M')}*
"""
        return prompt
    
    def generate_comprehensive_improvement_prompt(self) -> str:
        """Generate a comprehensive prompt covering all improvements."""
        total_issues = (len(self.analysis.critical_issues) + len(self.analysis.high_priority_issues) + 
                       len(self.analysis.medium_priority_issues))
        
        prompt = f"""# Comprehensive Improvement Plan - {self.project_name}

## Executive Summary
**Project Health**: {self.analysis.health_score}/100
**Total Issues Found**: {total_issues}
**Technology Stack**: {', '.join(self.analysis.tech_stack)}
**Analysis Date**: {datetime.fromisoformat(self.analysis.analysis_timestamp).strftime('%Y-%m-%d %H:%M')}

## ðŸ“Š Issue Breakdown
- ðŸš¨ **Critical**: {len(self.analysis.critical_issues)} issues
- âš ï¸ **High Priority**: {len(self.analysis.high_priority_issues)} issues  
- ðŸ“‹ **Medium Priority**: {len(self.analysis.medium_priority_issues)} issues
- ðŸ’¡ **Low Priority**: {len(self.analysis.low_priority_issues)} issues

## ðŸŽ¯ Strategic Recommendations
{chr(10).join(f'- {suggestion}' for suggestion in self.analysis.suggestions)}

## Phase 1: Critical Issues (Start Here)
{self._format_issues_for_prompt(self.analysis.critical_issues) if self.analysis.critical_issues else "âœ… No critical issues found!"}

## Phase 2: High Priority Issues  
{self._format_issues_for_prompt(self.analysis.high_priority_issues) if self.analysis.high_priority_issues else "âœ… No high priority issues found!"}

## Phase 3: Quality Improvements
{self._format_issues_for_prompt(self.analysis.medium_priority_issues[:5]) if self.analysis.medium_priority_issues else "âœ… Code quality looks good!"}

## Implementation Strategy

### Week 1: Stability & Critical Fixes
- Address all critical issues first
- Fix empty/stub files
- Resolve security vulnerabilities
- Ensure basic functionality works

### Week 2: Feature Completeness
- Implement missing core features
- Add proper error handling
- Complete test coverage gaps
- Address high-priority TODOs

### Week 3: Polish & Quality
- Code quality improvements
- Documentation updates
- Performance optimizations
- User experience enhancements

## Success Metrics
- [ ] Health score improved to 90+
- [ ] All critical and high-priority issues resolved
- [ ] Core functionality fully implemented
- [ ] Comprehensive test coverage
- [ ] Security vulnerabilities addressed
- [ ] Documentation updated and complete

## Technical Guidelines
1. **Maintain existing architecture** - Don't break current functionality
2. **Follow established patterns** - Stay consistent with existing code style
3. **Test incrementally** - Validate each fix before moving on
4. **Document changes** - Update README and inline documentation
5. **Security first** - Address all security issues before feature work

---
*This comprehensive plan will transform your project from {self.analysis.health_score}/100 to a production-ready codebase.*
"""
        return prompt
    
    def generate_test_improvement_prompt(self) -> str:
        """Generate prompt specifically for improving tests."""
        test_issues = [i for i in (self.analysis.critical_issues + self.analysis.high_priority_issues + 
                                  self.analysis.medium_priority_issues) if 'test' in i.type.lower()]
        
        prompt = f"""# Improve Testing Strategy - {self.project_name}

## Current Testing Status
**Project**: {self.project_name} ({self.analysis.project_type})
**Technology Stack**: {', '.join(self.analysis.tech_stack)}
**Health Score**: {self.analysis.health_score}/100

## ðŸ§ª Testing Issues Identified
{self._format_test_issues(test_issues) if test_issues else "No specific testing issues detected, but let's enhance your testing strategy."}

## Recommended Testing Strategy

### For {self.analysis.project_type.title()} Projects:
{self._get_testing_recommendations_by_type(self.analysis.project_type)}

### Implementation Plan:
1. **Set up testing framework** (if not already present)
2. **Write unit tests** for core business logic
3. **Add integration tests** for critical user flows
4. **Create component tests** (for frontend projects)
5. **Set up automated testing** in CI/CD pipeline

### Priority Test Areas:
{self._identify_priority_test_areas()}

## Testing Best Practices for Your Stack
{self._generate_stack_specific_testing_guidance()}

## Implementation Steps
1. **Assessment**: Review current test coverage
2. **Framework Setup**: Ensure proper testing tools are configured
3. **Core Tests**: Start with business logic and critical functions
4. **Integration**: Test component interactions and API calls
5. **E2E**: Add end-to-end tests for key user journeys
6. **Automation**: Set up continuous testing in your development workflow

---
*A robust testing strategy will significantly improve your project's reliability and maintainability.*
"""
        return prompt
    
    def _group_issues_by_type(self, issues: List[ProjectIssue]) -> Dict[str, List[ProjectIssue]]:
        """Group issues by their type."""
        groups = {}
        for issue in issues:
            if issue.type not in groups:
                groups[issue.type] = []
            groups[issue.type].append(issue)
        return groups
    
    def _format_issues_for_prompt(self, issues: List[ProjectIssue]) -> str:
        """Format issues for inclusion in prompts."""
        if not issues:
            return "No issues found in this category."
        
        formatted = []
        for i, issue in enumerate(issues, 1):
            location = ""
            if issue.file_path:
                location = f" in `{issue.file_path}`"
                if issue.line_number:
                    location += f" (line {issue.line_number})"
            
            formatted.append(f"""
### {i}. {issue.title}
**Description**: {issue.description}{location}
**Action**: {issue.suggested_action or 'Address this issue'}
""")
        
        return "\n".join(formatted)
    
    def _generate_action_plan(self, issues: List[ProjectIssue]) -> str:
        """Generate specific action plan for issues."""
        if not issues:
            return "No immediate actions required."
        
        actions = []
        for i, issue in enumerate(issues, 1):
            action = issue.suggested_action or f"Address {issue.title.lower()}"
            location = f" ({issue.file_path})" if issue.file_path else ""
            actions.append(f"{i}. **{action}**{location}")
        
        return "\n".join(actions)
    
    def _get_issue_context(self, issue: ProjectIssue) -> str:
        """Get additional context for a specific issue."""
        context_parts = []
        
        if issue.context:
            for key, value in issue.context.items():
                if key == 'file_size':
                    context_parts.append(f"**File Size**: {value} bytes")
                elif key == 'content_preview':
                    context_parts.append(f"**Content Preview**: `{value}`")
        
        if context_parts:
            return "## Additional Context\n" + "\n".join(context_parts) + "\n"
        
        return ""
    
    def _generate_specific_action_request(self, issue: ProjectIssue) -> str:
        """Generate specific action request based on issue type."""
        if issue.type == 'empty_file':
            return f"""Please implement the **{Path(issue.file_path).stem}** component/module that is currently empty or stub.
            
**Specific requirements:**
- Follow the existing code patterns in the project
- Implement the full functionality this file is meant to provide
- Add proper TypeScript types and interfaces
- Include error handling and validation
- Match the coding style of similar files in the project"""
        
        elif issue.type == 'todo':
            return f"""Please address the {issue.title.split()[0]} comment: "{issue.description}"
            
**Specific requirements:**
- Implement the functionality described in the comment
- Remove or update the comment once resolved
- Ensure the implementation follows project conventions
- Add tests if the change affects functionality"""
        
        elif issue.type == 'missing_feature':
            return f"""Please implement {issue.title} functionality in the project.
            
**Specific requirements:**
- Research best practices for {issue.title} in {self.analysis.project_type} projects
- Implement following the established patterns in the codebase
- Ensure integration with existing functionality
- Add appropriate tests and documentation"""
        
        elif issue.type == 'security':
            return f"""Please fix the security vulnerability: {issue.description}
            
**Specific requirements:**
- Remove or properly secure the identified vulnerability
- Follow security best practices for {', '.join(self.analysis.tech_stack)}
- Ensure the fix doesn't break existing functionality
- Consider adding security tests to prevent regression"""
        
        else:
            return f"""Please address this issue: {issue.description}
            
**Specific requirements:**
- Follow the project's existing patterns and conventions
- Ensure the fix is complete and thoroughly tested
- Update documentation if necessary"""
    
    def _generate_implementation_requirements(self, issue: ProjectIssue) -> str:
        """Generate implementation requirements for the issue."""
        base_requirements = f"""
1. **Architecture Compatibility**: Ensure changes work with existing {self.analysis.project_type} architecture
2. **Technology Stack**: Use the established {', '.join(self.analysis.tech_stack)} patterns
3. **Code Quality**: Follow the project's coding standards and patterns
4. **Testing**: Add or update tests as appropriate
5. **Documentation**: Update inline documentation and comments"""
        
        if issue.type == 'empty_file' and '.tsx' in (issue.file_path or ''):
            base_requirements += """
6. **React Patterns**: Follow existing component patterns and hooks usage
7. **TypeScript**: Use proper typing and interfaces
8. **Styling**: Match existing styling approach and theme"""
        
        return base_requirements
    
    def _generate_acceptance_criteria(self, issue: ProjectIssue) -> str:
        """Generate acceptance criteria for the issue."""
        criteria = [
            "âœ… Issue is completely resolved",
            "âœ… No new bugs or regressions introduced", 
            "âœ… Code follows project conventions and patterns",
            "âœ… Changes are properly tested"
        ]
        
        if issue.type == 'empty_file':
            criteria.extend([
                "âœ… File implements the intended functionality",
                "âœ… Component integrates properly with the application",
                "âœ… Proper error handling is included"
            ])
        
        elif issue.type == 'todo':
            criteria.extend([
                "âœ… TODO comment is addressed and removed/updated",
                "âœ… Implementation matches the intent of the comment"
            ])
        
        elif issue.type == 'security':
            criteria.extend([
                "âœ… Security vulnerability is completely fixed",
                "âœ… No similar vulnerabilities remain in the codebase"
            ])
        
        return "\n".join(criteria)
    
    def _get_file_count(self) -> str:
        """Get approximate file count from context."""
        if self.context and 'code_structure' in self.context:
            file_count = self.context['code_structure'].get('file_count', 'multiple')
            return f"{file_count}" if isinstance(file_count, int) else "multiple"
        return "multiple"
    
    def _generate_no_critical_issues_prompt(self) -> str:
        """Generate prompt when no critical issues are found."""
        return f"""# Excellent! No Critical Issues Found - {self.project_name}

## Project Health Report
**Health Score**: {self.analysis.health_score}/100 âœ…
**Status**: Your project is in good health!
**Technology Stack**: {', '.join(self.analysis.tech_stack)}

## Current Status Summary
- ðŸš¨ **Critical Issues**: 0 (Excellent!)
- âš ï¸ **High Priority**: {len(self.analysis.high_priority_issues)} issues
- ðŸ“‹ **Medium Priority**: {len(self.analysis.medium_priority_issues)} issues

## Recommendations for Continued Excellence
{chr(10).join(f'- {suggestion}' for suggestion in self.analysis.suggestions)}

## Optional Improvements
{self._format_issues_for_prompt(self.analysis.high_priority_issues[:3]) if self.analysis.high_priority_issues else "Consider adding advanced features or performance optimizations."}

## Keep Up the Great Work!
Your {self.project_name} project demonstrates good development practices. Focus on the high-priority items when you have time, but there's nothing blocking or critical that needs immediate attention.

---
*Project health analysis complete - you're doing great!*
"""

    def _format_missing_features(self, missing_features: List[ProjectIssue]) -> str:
        """Format missing features for prompt."""
        if not missing_features:
            return "No missing features detected - your project has good feature coverage!"
        
        formatted = []
        for i, feature in enumerate(missing_features, 1):
            formatted.append(f"""
### {i}. {feature.title}
**Impact**: {feature.description}
**Implementation**: {feature.suggested_action}
""")
        
        return "\n".join(formatted)
    
    def _prioritize_missing_features(self, features: List[ProjectIssue], phase: str) -> str:
        """Prioritize missing features by implementation phase."""
        priorities = {
            'core': ['Error Handling', 'Logging', 'Authentication', 'Input Validation'],
            'ux': ['Loading States', 'Error Boundaries', 'User Feedback'],
            'quality': ['Testing', 'Documentation', 'Type Hints', 'Security Headers']
        }
        
        phase_features = []
        for feature in features:
            for priority_feature in priorities.get(phase, []):
                if priority_feature.lower() in feature.title.lower():
                    phase_features.append(f"- **{feature.title}**: {feature.suggested_action}")
        
        if not phase_features:
            return f"- No {phase} features identified for this phase"
        
        return "\n".join(phase_features)
    
    def _generate_advanced_features_suggestions(self) -> str:
        """Generate advanced feature suggestions for healthy projects."""
        suggestions = {
            'react': [
                "ðŸš€ **Progressive Web App (PWA)** capabilities for offline usage",
                "ðŸ“Š **Advanced Analytics** and user behavior tracking", 
                "ðŸŽ¨ **Theme System** with dark/light mode support",
                "ðŸ”„ **Real-time Features** with WebSocket integration",
                "ðŸ“± **Mobile Optimization** and responsive design enhancements"
            ],
            'python': [
                "âš¡ **Performance Optimization** with async/await patterns",
                "ðŸ³ **Containerization** with Docker for easy deployment",
                "ðŸ“ˆ **Monitoring & Metrics** with detailed logging and alerts",
                "ðŸ”Œ **Plugin Architecture** for extensibility",
                "ðŸ”’ **Advanced Security** with OAuth and rate limiting"
            ]
        }
        
        project_suggestions = suggestions.get(self.analysis.project_type, [
            "ðŸš€ Performance optimizations and monitoring",
            "ðŸ”’ Enhanced security features",
            "ðŸ“Š Advanced analytics and reporting",
            "ðŸŽ¨ User experience improvements",
            "ðŸ”§ Developer experience enhancements"
        ])
        
        return "\n".join(project_suggestions)
    
    def _format_test_issues(self, test_issues: List[ProjectIssue]) -> str:
        """Format test-related issues."""
        if not test_issues:
            return ""
        
        formatted = []
        for issue in test_issues:
            formatted.append(f"- **{issue.title}**: {issue.description}")
        
        return "\n".join(formatted)
    
    def _get_testing_recommendations_by_type(self, project_type: str) -> str:
        """Get testing recommendations based on project type."""
        recommendations = {
            'react': """
**Unit Tests**: Test individual components with Jest and React Testing Library
**Integration Tests**: Test component interactions and API calls  
**E2E Tests**: Use Cypress or Playwright for full user journey testing
**Visual Tests**: Consider Storybook for component documentation and testing""",
            'python': """
**Unit Tests**: Use pytest for function and class testing
**Integration Tests**: Test database interactions and external APIs
**Property Tests**: Consider hypothesis for property-based testing
**Performance Tests**: Add benchmarking for critical algorithms""",
            'node': """
**Unit Tests**: Test individual functions and modules with Jest
**API Tests**: Test endpoints with supertest or similar tools
**Integration Tests**: Test database and external service interactions
**Load Tests**: Consider performance testing for high-traffic endpoints"""
        }
        
        return recommendations.get(project_type, """
**Unit Tests**: Test individual functions and components
**Integration Tests**: Test component interactions
**End-to-End Tests**: Test complete user workflows
**Performance Tests**: Monitor and test critical performance metrics""")
    
    def _identify_priority_test_areas(self) -> str:
        """Identify priority areas for testing based on project analysis."""
        areas = []
        
        # Check for critical components that need testing
        empty_files = [i for i in self.analysis.critical_issues + self.analysis.high_priority_issues 
                      if i.type == 'empty_file']
        if empty_files:
            areas.append("ðŸš¨ **Newly implemented components** (test empty files after implementation)")
        
        # Core business logic
        areas.append("ðŸ’¼ **Core business logic** and data processing functions")
        areas.append("ðŸ”Œ **API integrations** and external service interactions")
        areas.append("ðŸŽ¯ **User authentication** and authorization flows")
        areas.append("ðŸ›¡ï¸ **Input validation** and error handling")
        
        return "\n".join(areas)
    
    def _generate_stack_specific_testing_guidance(self) -> str:
        """Generate testing guidance specific to the technology stack."""
        if 'React' in self.analysis.tech_stack:
            return """
**React Testing Best Practices:**
- Use React Testing Library for component testing
- Test user interactions, not implementation details
- Mock external dependencies and API calls
- Test accessibility with screen readers in mind
- Use MSW (Mock Service Worker) for API mocking"""
        
        elif 'Python' in self.analysis.tech_stack:
            return """
**Python Testing Best Practices:**
- Use pytest with fixtures for test data
- Mock external dependencies with unittest.mock
- Test edge cases and error conditions
- Use parametrized tests for multiple inputs
- Include docstring examples that can be tested with doctest"""
        
        else:
            return """
**General Testing Best Practices:**
- Write tests that are maintainable and readable
- Test behavior, not implementation details
- Mock external dependencies to isolate units
- Include both positive and negative test cases
- Keep tests fast and independent of each other"""